{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import sklearn.metrics as sk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='./data/imdb.train'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\W+', ' ', line).strip().lower()  # perhaps don't make words lowercase?\n",
    "            x.append(line[:-1])\n",
    "            x[-1] = ' '.join(word for word in x[-1].split() if word not in stop_words)\n",
    "            y.append(line[-1])\n",
    "    return x, np.array(y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_rank(dataset, _vocab, desired_vocab_size=5000):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "    :vocab: a _ordered_ dictionary of vocab words and counts from get_vocab\n",
    "    :param desired_vocab_size: the desired vocabulary size\n",
    "    words no longer in vocab become UUUNNNKKK\n",
    "    :return: the text corpus with words mapped to their vocab rank,\n",
    "    with all sufficiently infrequent words mapped to UUUNNNKKK; UUUNNNKKK has rank desired_vocab_size\n",
    "    (the infrequent word cutoff is determined by desired_vocab size)\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    vocab_ordered = list(_vocab)\n",
    "    count_cutoff = _vocab[vocab_ordered[desired_vocab_size-1]] # get word by its rank and map to its count\n",
    "    \n",
    "    word_to_rank = {}\n",
    "    for i in range(len(vocab_ordered)):\n",
    "        # we add one to make room for any future padding symbol with value 0\n",
    "        word_to_rank[vocab_ordered[i]] = i + 1\n",
    "    \n",
    "    # we need to ensure that other words below the word on the edge of our desired_vocab size\n",
    "    # are not also on the count cutoff, so we subtract a bit\n",
    "    # this is likely quicker than adding another preventative if case\n",
    "    for i in range(50):\n",
    "        _vocab[vocab_ordered[desired_vocab_size+i]] -= 0.1\n",
    "    \n",
    "    for i in range(len(_dataset)):\n",
    "        example = _dataset[i]\n",
    "        example_as_list = example.split()\n",
    "        for j in range(len(example_as_list)):\n",
    "            try:\n",
    "                if _vocab[example_as_list[j]] >= count_cutoff:\n",
    "                    example_as_list[j] = word_to_rank[example_as_list[j]] \n",
    "                else:\n",
    "                    example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "            except:\n",
    "                example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "        _dataset[i] = example_as_list\n",
    "\n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from keras\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    '''Pads each sequence to the same length:\n",
    "    the length of the longest sequence.\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "max_example_len = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "vocab_size = 5000\n",
    "num_epochs = 15\n",
    "\n",
    "print('Loading Data')\n",
    "X_train, Y_train = load_data()\n",
    "X_dev, Y_dev = load_data('./data/imdb.dev')\n",
    "X_test, Y_test = load_data('./data/imdb.test')\n",
    "\n",
    "vocab = get_vocab(X_train)\n",
    "X_train = text_to_rank(X_train, vocab, 5000)\n",
    "X_dev = text_to_rank(X_dev, vocab, 5000)\n",
    "X_test = text_to_rank(X_test, vocab, 5000)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_example_len)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_example_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_example_len)\n",
    "print('Data loaded')\n",
    "\n",
    "num_examples = Y_train.shape[0]\n",
    "num_batches = num_examples//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(dtype=tf.int32, shape=[None, max_example_len])\n",
    "    y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "#     is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # add one to vocab size for the padding symbol\n",
    "    W_embedding = tf.Variable(tf.nn.l2_normalize(\n",
    "        tf.random_normal([vocab_size+1, embedding_dims]), 0), trainable=True)\n",
    "    \n",
    "    w_vecs = tf.nn.embedding_lookup(W_embedding, x)\n",
    "    pooled = tf.reduce_mean(w_vecs, reduction_indices=[1])\n",
    "    \n",
    "    W_out = tf.Variable(tf.nn.l2_normalize(tf.random_normal([embedding_dims, 2]), 0))\n",
    "    b_out = tf.Variable(tf.zeros([2]))\n",
    "    \n",
    "    logits = tf.matmul(pooled, W_out) + b_out\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(logits, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "tf.initialize_all_variables().run()\n",
    "# create saver to train model\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Minibatch loss 0.529 | Minibatch accuracy 68.750 | Dev accuracy 59.560\n",
      "Epoch 2 | Minibatch loss 0.251 | Minibatch accuracy 90.625 | Dev accuracy 87.000\n",
      "Epoch 3 | Minibatch loss 0.301 | Minibatch accuracy 84.375 | Dev accuracy 86.800\n",
      "Epoch 4 | Minibatch loss 0.175 | Minibatch accuracy 96.875 | Dev accuracy 84.840\n",
      "Epoch 5 | Minibatch loss 0.371 | Minibatch accuracy 84.375 | Dev accuracy 85.740\n",
      "Epoch 6 | Minibatch loss 0.234 | Minibatch accuracy 84.375 | Dev accuracy 85.540\n",
      "Epoch 7 | Minibatch loss 0.248 | Minibatch accuracy 90.625 | Dev accuracy 88.840\n",
      "Epoch 8 | Minibatch loss 0.277 | Minibatch accuracy 87.500 | Dev accuracy 83.520\n",
      "Epoch 9 | Minibatch loss 0.293 | Minibatch accuracy 87.500 | Dev accuracy 89.260\n",
      "Epoch 10 | Minibatch loss 0.225 | Minibatch accuracy 93.750 | Dev accuracy 88.880\n",
      "Epoch 11 | Minibatch loss 0.252 | Minibatch accuracy 84.375 | Dev accuracy 85.760\n",
      "Epoch 12 | Minibatch loss 0.215 | Minibatch accuracy 93.750 | Dev accuracy 87.980\n",
      "Epoch 13 | Minibatch loss 0.159 | Minibatch accuracy 90.625 | Dev accuracy 89.080\n",
      "Epoch 14 | Minibatch loss 0.188 | Minibatch accuracy 93.750 | Dev accuracy 86.380\n",
      "Epoch 15 | Minibatch loss 0.157 | Minibatch accuracy 93.750 | Dev accuracy 87.220\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle data every epoch\n",
    "    indices = np.arange(num_examples)\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    Y_train = Y_train[indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "\n",
    "        x_batch = X_train[offset:offset + batch_size]\n",
    "        y_batch = Y_train[offset:offset + batch_size]\n",
    "\n",
    "        _, l, batch_acc = sess.run([optimizer, loss, acc], feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            curr_dev_acc = sess.run(\n",
    "                acc, feed_dict={x: X_dev, y: Y_dev})\n",
    "            if best_acc < curr_dev_acc:\n",
    "                best_acc = curr_dev_acc\n",
    "                saver.save(sess, './data/best_imdb_model.ckpt')\n",
    "\n",
    "    print('Epoch %d | Minibatch loss %.3f | Minibatch accuracy %.3f | Dev accuracy %.3f' %\n",
    "          (epoch+1, l, batch_acc, curr_dev_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model restored!\n",
      "Dev accuracy: 89.26\n"
     ]
    }
   ],
   "source": [
    "# restore variables from disk\n",
    "saver.restore(sess, \"./data/best_imdb_model.ckpt\")\n",
    "print(\"Best model restored!\")\n",
    "\n",
    "print('Dev accuracy:', sess.run(acc, feed_dict={x: X_dev, y: Y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(2.) + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(2.) + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(2.) + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN DEV STATS\n",
      "IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "10.74 | 0.884663 0.137912 | 0.901951 0.125093 | 0.740977 0.154629\n",
      "\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR 0.966338843302\n",
      "AUROC 0.795715318712\n",
      "END DEV STATS\n"
     ]
    }
   ],
   "source": [
    "print('BEGIN DEV STATS')\n",
    "err, scores, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, s, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: X_dev, y: Y_dev})\n",
    "\n",
    "print('IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "print('\\nKL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "# safe, risky = s_rp, s_wp\n",
    "# labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "# labels[:safe.shape[0]] += 1\n",
    "# examples = np.squeeze(np.vstack((safe, risky)))\n",
    "# print('\\nPrediction Prob: Right/Wrong classification distinction')\n",
    "# print('AUPR', sk.average_precision_score(labels, examples))\n",
    "# print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "print('END DEV STATS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, taus = sk.roc_curve(labels, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1074"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds != Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(kl_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau_star = -1e5\n",
    "tau_idx = -1\n",
    "J_stat = -1e-5\n",
    "\n",
    "for i in range(fpr.shape[0]):\n",
    "    curr_J = tpr[i] + (1 - fpr[i]) - 1\n",
    "    if curr_J > J_stat:\n",
    "        J_stat = curr_J\n",
    "        tau_star = taus[i]\n",
    "        tau_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32833761"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16120000000000001"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_prime = preds.copy()\n",
    "preds_prime[(kl_a < 0.1).reshape((-1,))] = 1 - preds[(kl_a < 0.1).reshape((-1,))]\n",
    "\n",
    "np.mean(preds_prime != Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.159"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kl_a < 0.1).reshape((-1,)).sum()/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4806 0.664964\n"
     ]
    }
   ],
   "source": [
    "min_err = 100\n",
    "best_t = 1e5\n",
    "for t in taus:\n",
    "    v = np.mean((s_p > t).astype(np.int).reshape((-1)) != Y_dev)\n",
    "    if v < min_err:\n",
    "        min_err = v\n",
    "        best_t = t\n",
    "print(min_err, best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49740000000000001"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((kl_a > tau_star).astype(np.int).reshape((-1)) != Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.881059 298\n"
     ]
    }
   ],
   "source": [
    "print(tau_star, tau_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_err = 100\n",
    "for t in range(0,100):\n",
    "    v = np.mean((scores[:,1] > t/100).astype(np.int).reshape((-1,)) != Y_dev)\n",
    "    if v < min_err:\n",
    "        min_err = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.119999999999997"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean((scores[:,1] > tau_star).astype(np.int).reshape((-1)) != Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98233402],\n",
       "       [ 0.72248143],\n",
       "       [ 0.87506396],\n",
       "       ..., \n",
       "       [ 0.91968423],\n",
       "       [ 0.69464648],\n",
       "       [ 0.61481184]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEVCAYAAADOwrOnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e68088fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(kl_r, color='blue')\n",
    "plt.hist(kl_w, color='red')\n",
    "plt.title('DevTest Example KL Divergence Histograms (unnormed)')\n",
    "plt.xlabel('KL[softmax dist||uniform]')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(['Divergences of Correctly Classified Examples', 'Divergences of Incorrectly Classified Examples'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "11.84 | 0.878831 0.13987 | 0.89978 0.124596 | 0.722847 0.148468\n",
      "\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR 0.968489887534\n",
      "AUROC 0.821134991048\n",
      "\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR 0.968489843164\n",
      "AUROC 0.821134968056\n"
     ]
    }
   ],
   "source": [
    "err, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: X_test, y: Y_test})\n",
    "\n",
    "print('IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "print('\\nKL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('\\nPrediction Prob: Right/Wrong classification distinction')\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cr_data, cr_labels = load_data('./data/CR.test')\n",
    "cr_data = text_to_rank(cr_data, vocab, 5000)\n",
    "cr_data = pad_sequences(cr_data, maxlen=max_example_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Reviews (out of sample) Prediction Probability (mean, std):\n",
      "0.6191 0.0908484\n",
      "\n",
      "KL[p||u]: In/out distribution distinction\n",
      "AUPR 0.998346269866\n",
      "AUROC 0.92270688\n",
      "\n",
      "Prediction Prob: In/out distribution distinction\n",
      "AUPR 0.99834626832\n",
      "AUROC 0.9227068\n",
      "\n",
      "KL[p||u]: In/out distribution distinction (relative to correct examples)\n",
      "AUPR 0.998743855974\n",
      "AUROC 0.947301487933\n",
      "\n",
      "Prediction Prob: In/out distribution distinction (relative to correct examples)\n",
      "AUPR 0.998743855974\n",
      "AUROC 0.947301487933\n"
     ]
    }
   ],
   "source": [
    "kl_oos, s_p_oos = sess.run([kl_all, s_prob], feed_dict={x: cr_data})\n",
    "\n",
    "print('Customer Reviews (out of sample) Prediction Probability (mean, std):')\n",
    "print(np.mean(s_p_oos), np.std(s_p_oos))\n",
    "\n",
    "print('\\nKL[p||u]: In/out distribution distinction')\n",
    "in_sample, oos = kl_a, kl_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nPrediction Prob: In/out distribution distinction')\n",
    "in_sample, oos = s_p, s_p_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nKL[p||u]: In/out distribution distinction (relative to correct examples)')\n",
    "in_sample, oos = kl_r, kl_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nPrediction Prob: In/out distribution distinction (relative to correct examples)')\n",
    "in_sample, oos = s_rp, s_p_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_data, rt_labels = load_data('./data/MR.test')\n",
    "rt_data = text_to_rank(rt_data, vocab, 5000)\n",
    "rt_data = pad_sequences(rt_data, maxlen=max_example_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotten Tomatoes (out of sample) Prediction Probability (mean, std):\n",
      "0.630085 0.0969686\n",
      "\n",
      "KL[p||u]: In/out distribution distinction\n",
      "AUPR 0.996352671834\n",
      "AUROC 0.9144708\n",
      "\n",
      "Prediction Prob: In/out distribution distinction\n",
      "AUPR 0.996352668118\n",
      "AUROC 0.9144707\n",
      "\n",
      "KL[p||u]: In/out distribution distinction (relative to correct examples)\n",
      "AUPR 0.997195570054\n",
      "AUROC 0.940886703865\n",
      "\n",
      "Prediction Prob: In/out distribution distinction (relative to correct examples)\n",
      "AUPR 0.997195568168\n",
      "AUROC 0.940886658501\n"
     ]
    }
   ],
   "source": [
    "kl_oos, s_p_oos = sess.run([kl_all, s_prob], feed_dict={x: rt_data})\n",
    "\n",
    "print('Rotten Tomatoes (out of sample) Prediction Probability (mean, std):')\n",
    "print(np.mean(s_p_oos), np.std(s_p_oos))\n",
    "\n",
    "print('\\nKL[p||u]: In/out distribution distinction')\n",
    "in_sample, oos = kl_a, kl_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nPrediction Prob: In/out distribution distinction')\n",
    "in_sample, oos = s_p, s_p_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nKL[p||u]: In/out distribution distinction (relative to correct examples)')\n",
    "in_sample, oos = kl_r, kl_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))\n",
    "\n",
    "print('\\nPrediction Prob: In/out distribution distinction (relative to correct examples)')\n",
    "in_sample, oos = s_rp, s_p_oos\n",
    "labels = np.zeros((in_sample.shape[0] + oos.shape[0]), dtype=np.int32)\n",
    "labels[:in_sample.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((in_sample, oos)))\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
